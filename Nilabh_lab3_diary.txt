% #1 - defining a mini web 
echo off 

A =

     1     0     1     0     0     0     1
     1     1     0     0     1     1     0
     1     0     1     0     0     1     0
     1     1     1     1     0     1     0
     0     1     1     1     1     1     1
     0     0     0     1     1     1     0
     0     1     0     1     0     1     1


P =

    0.2500         0    0.2500         0         0         0    0.3333
    0.2500    0.2500         0         0    0.3333    0.1667         0
    0.2500         0    0.2500         0         0    0.1667         0
    0.2500    0.2500    0.2500    0.2500         0    0.1667         0
         0    0.2500    0.2500    0.2500    0.3333    0.1667    0.3333
         0         0         0    0.2500    0.3333    0.1667         0
         0    0.2500         0    0.2500         0    0.1667    0.3333

% #2 - Crawling the web from site 1 and printing "location" at timestep 1
% and 2
echo off

x1 =

    0.2500
    0.2500
    0.2500
    0.2500
         0
         0
         0


x2 =

    0.1250
    0.1250
    0.1250
    0.2500
    0.1875
    0.0625
    0.1250

% #3 - printing location of crawlers starting at site one for different
% time steps using for loop
echo off 
x5 
    0.0888
    0.1694
    0.0639
    0.1401
    0.2474
    0.1412
    0.1491

x10 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x20 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x50 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

% #4 - changing starting vector to being equally distributed and repeating
% process from #3 to observe changes in behavior based on new location
% vectors. 
echo off 
x1 
    0.8333
    1.0000
    0.6667
    1.1667
    1.5833
    0.7500
    1.0000

x2 
    0.7083
    1.1111
    0.5000
    1.0417
    1.6944
    0.9444
    1.0000

x10 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x20 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x50 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

% #5 - computing steady state q of probability matrix, aka the page rank
% vector. q is then normalized to sum to 7, the number of sites in our mini
% web. The site with the highest page rank is site 5, while the one with
% the lowest page rank is site 3. 
echo off 

q =

    0.6123
    1.2010
    0.4240
    0.9655
    1.7484
    0.9891
    1.0596

5 has the highest page rank 
3 has the lowest page rank 
% #6 - finding how we get highest page rank
% 6a. finding in degree nodes by multiplying adjacency matrix by column
% vector of ones (sums of each row) 
% 6b. Finding out degree nodes by getting sums of each row
% 6c. sorting page rank and din and dout in descending order 
% seems to show that the page rank has a strong positive association with
% the number of in degree nodes and a slight negative association with out
% degree nodes 
echo off 

din =

     3
     4
     3
     5
     6
     3
     4


dout =

     4     4     4     4     3     6     3


qsorted =

    1.7484
    1.2010
    1.0596
    0.9891
    0.9655
    0.6123
    0.4240


Ivec =

     5
     2
     7
     6
     4
     1
     3


din_sorted =

     6
     4
     4
     3
     5
     3
     3


dout_sorted =

     3     4     3     6     4     4     4

% #7 - The site with the lowest page rank is site 3, as discussed earlier.
% Therefore, to raise the page rank of this website, we will add an edge
% between the 5th website. with the highest page rank, and the 3rd website,
% as this would increase the in degree nodes while connecting from the most
% popular website in our mini-web. From the results, we can see that the
% page rank of the 3rd website goes from last to 4th, meaning this change
% worked. Below. I print the steady state vector r and the sorted version.  
echo off 

r =

    0.2698
    0.3469
    0.3469
    0.3854
    0.5781
    0.2891
    0.3469


rsorted =

    0.5781
    0.3854
    0.3469
    0.3469
    0.3469
    0.2891
    0.2698


Pvec =

     5
     4
     2
     3
     7
     6
     1

% #8 - Adding a damping factor and finding steady state vectors for
% resulting P-hat matrices. The results show that adding a higher dampening
% factor results in the pagerank values getting closer, which concurs with
% the idea of web crawlers choosing sites randomly. 
echo off 

ans =

    0.1017
    0.1617
    0.0792
    0.1442
    0.2294
    0.1364
    0.1476


ans =

    0.0925
    0.1679
    0.0671
    0.1404
    0.2427
    0.1394
    0.1499


ans =

    0.1096
    0.1567
    0.0899
    0.1468
    0.2170
    0.1343
    0.1459

Nilabh_lab3_script
% #1 - defining a mini web 
echo off 

A =

     1     0     1     0     0     0     1
     1     1     0     0     1     1     0
     1     0     1     0     0     1     0
     1     1     1     1     0     1     0
     0     1     1     1     1     1     1
     0     0     0     1     1     1     0
     0     1     0     1     0     1     1


P =

    0.2500         0    0.2500         0         0         0    0.3333
    0.2500    0.2500         0         0    0.3333    0.1667         0
    0.2500         0    0.2500         0         0    0.1667         0
    0.2500    0.2500    0.2500    0.2500         0    0.1667         0
         0    0.2500    0.2500    0.2500    0.3333    0.1667    0.3333
         0         0         0    0.2500    0.3333    0.1667         0
         0    0.2500         0    0.2500         0    0.1667    0.3333

% #2 - Crawling the web from site 1 and printing "location" at timestep 1
% and 2
echo off

x1 =

    0.2500
    0.2500
    0.2500
    0.2500
         0
         0
         0


x2 =

    0.1250
    0.1250
    0.1250
    0.2500
    0.1875
    0.0625
    0.1250

% #3 - printing location of crawlers starting at site one for different
% time steps using for loop
echo off 
x5 
    0.0888
    0.1694
    0.0639
    0.1401
    0.2474
    0.1412
    0.1491

x10 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x20 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x50 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

% #4 - changing starting vector to being equally distributed and repeating
% process from #3 to observe changes in behavior based on new location
% vectors. 
echo off 
x1 
    0.8333
    1.0000
    0.6667
    1.1667
    1.5833
    0.7500
    1.0000

x2 
    0.7083
    1.1111
    0.5000
    1.0417
    1.6944
    0.9444
    1.0000

x10 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x20 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x50 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

% #5 - computing steady state q of probability matrix, aka the page rank
% vector. q is then normalized to sum to 7, the number of sites in our mini
% web. The site with the highest page rank is site 5, while the one with
% the lowest page rank is site 3. 
echo off 

q =

    0.6123
    1.2010
    0.4240
    0.9655
    1.7484
    0.9891
    1.0596

5 has the highest page rank 
3 has the lowest page rank 
% #6 - finding how we get highest page rank
% 6a. finding in degree nodes by multiplying adjacency matrix by column
% vector of ones (sums of each row) 
% 6b. Finding out degree nodes by getting sums of each row
% 6c. sorting page rank and din and dout in descending order 
% seems to show that the page rank has a strong positive association with
% the number of in degree nodes and a slight negative association with out
% degree nodes 
echo off 

din =

     3
     4
     3
     5
     6
     3
     4


dout =

     4     4     4     4     3     6     3


qsorted =

    1.7484
    1.2010
    1.0596
    0.9891
    0.9655
    0.6123
    0.4240


Ivec =

     5
     2
     7
     6
     4
     1
     3


din_sorted =

     6
     4
     4
     3
     5
     3
     3


dout_sorted =

     3     4     3     6     4     4     4

% #7 - The site with the lowest page rank is site 3, as discussed earlier.
% Therefore, to raise the page rank of this website, we will add an edge
% between the 5th website. with the highest page rank, and the 3rd website,
% as this would increase the in degree nodes while connecting from the most
% popular website in our mini-web. From the results, we can see that the
% page rank of the 3rd website goes from last to 4th, meaning this change
% worked. Below. I print the steady state vector r and the sorted version.  
echo off 

r =

    0.2698
    0.3469
    0.3469
    0.3854
    0.5781
    0.2891
    0.3469


rsorted =

    0.5781
    0.3854
    0.3469
    0.3469
    0.3469
    0.2891
    0.2698


Pvec =

     5
     4
     2
     3
     7
     6
     1

% #8 - Adding a damping factor and finding steady state vectors for
% resulting P-hat matrices. The results show that adding a higher dampening
% factor results in the pagerank values getting closer, which concurs with
% the idea of web crawlers choosing sites randomly. 
echo off 

q_85 =

   -1.7988
   -2.8611
   -1.4009
   -2.5510
   -4.0583
   -2.4129
   -2.6108


q_95 =

   -1.6104
   -2.9226
   -1.1684
   -2.4427
   -4.2239
   -2.4261
   -2.6088


q_75 =

   -1.9640
   -2.8076
   -1.6112
   -2.6309
   -3.8883
   -2.4062
   -2.6141

Nilabh_lab3_script
% #1 - defining a mini web 
echo off 

A =

     1     0     1     0     0     0     1
     1     1     0     0     1     1     0
     1     0     1     0     0     1     0
     1     1     1     1     0     1     0
     0     1     1     1     1     1     1
     0     0     0     1     1     1     0
     0     1     0     1     0     1     1


P =

    0.2500         0    0.2500         0         0         0    0.3333
    0.2500    0.2500         0         0    0.3333    0.1667         0
    0.2500         0    0.2500         0         0    0.1667         0
    0.2500    0.2500    0.2500    0.2500         0    0.1667         0
         0    0.2500    0.2500    0.2500    0.3333    0.1667    0.3333
         0         0         0    0.2500    0.3333    0.1667         0
         0    0.2500         0    0.2500         0    0.1667    0.3333

% #2 - Crawling the web from site 1 and printing "location" at timestep 1
% and 2
echo off

x1 =

    0.2500
    0.2500
    0.2500
    0.2500
         0
         0
         0


x2 =

    0.1250
    0.1250
    0.1250
    0.2500
    0.1875
    0.0625
    0.1250

% #3 - printing location of crawlers starting at site one for different
% time steps using for loop
echo off 
x5 
    0.0888
    0.1694
    0.0639
    0.1401
    0.2474
    0.1412
    0.1491

x10 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x20 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x50 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

% #4 - changing starting vector to being equally distributed and repeating
% process from #3 to observe changes in behavior based on new location
% vectors. 
echo off 
x1 
    0.8333
    1.0000
    0.6667
    1.1667
    1.5833
    0.7500
    1.0000

x2 
    0.7083
    1.1111
    0.5000
    1.0417
    1.6944
    0.9444
    1.0000

x10 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x20 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x50 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

% #5 - computing steady state q of probability matrix, aka the page rank
% vector. q is then normalized to sum to 7, the number of sites in our mini
% web. The site with the highest page rank is site 5, while the one with
% the lowest page rank is site 3. 
echo off 

q =

    0.6123
    1.2010
    0.4240
    0.9655
    1.7484
    0.9891
    1.0596

5 has the highest page rank 
3 has the lowest page rank 
% #6 - finding how we get highest page rank
% 6a. finding in degree nodes by multiplying adjacency matrix by column
% vector of ones (sums of each row) 
% 6b. Finding out degree nodes by getting sums of each row
% 6c. sorting page rank and din and dout in descending order 
% seems to show that the page rank has a strong positive association with
% the number of in degree nodes and a slight negative association with out
% degree nodes 
echo off 

din =

     3
     4
     3
     5
     6
     3
     4


dout =

     4     4     4     4     3     6     3


qsorted =

    1.7484
    1.2010
    1.0596
    0.9891
    0.9655
    0.6123
    0.4240


Ivec =

     5
     2
     7
     6
     4
     1
     3


din_sorted =

     6
     4
     4
     3
     5
     3
     3


dout_sorted =

     3     4     3     6     4     4     4

% #7 - The site with the lowest page rank is site 3, as discussed earlier.
% Therefore, to raise the page rank of this website, we will add an edge
% between the 5th website. with the highest page rank, and the 3rd website,
% as this would increase the in degree nodes while connecting from the most
% popular website in our mini-web. From the results, we can see that the
% page rank of the 3rd website goes from last to 4th, meaning this change
% worked. Below. I print the steady state vector r and the sorted version.  
echo off 

r =

    0.2698
    0.3469
    0.3469
    0.3854
    0.5781
    0.2891
    0.3469


rsorted =

    0.5781
    0.3854
    0.3469
    0.3469
    0.3469
    0.2891
    0.2698


Pvec =

     5
     4
     2
     3
     7
     6
     1

% #8 - Adding a damping factor and finding steady state vectors for
% resulting P-hat matrices. The results show that adding a higher dampening
% factor results in the pagerank values getting closer, which concurs with
% the idea of web crawlers choosing sites randomly. 
echo off 

q_85 =

   -0.2570
   -0.4087
   -0.2001
   -0.3644
   -0.5798
   -0.3447
   -0.3730


q_95 =

   -0.2301
   -0.4175
   -0.1669
   -0.3490
   -0.6034
   -0.3466
   -0.3727


q_75 =

   -0.2806
   -0.4011
   -0.2302
   -0.3758
   -0.5555
   -0.3437
   -0.3734

Nilabh_lab3_script
% #1 - defining a mini web 
echo off 

A =

     1     0     1     0     0     0     1
     1     1     0     0     1     1     0
     1     0     1     0     0     1     0
     1     1     1     1     0     1     0
     0     1     1     1     1     1     1
     0     0     0     1     1     1     0
     0     1     0     1     0     1     1


P =

    0.2500         0    0.2500         0         0         0    0.3333
    0.2500    0.2500         0         0    0.3333    0.1667         0
    0.2500         0    0.2500         0         0    0.1667         0
    0.2500    0.2500    0.2500    0.2500         0    0.1667         0
         0    0.2500    0.2500    0.2500    0.3333    0.1667    0.3333
         0         0         0    0.2500    0.3333    0.1667         0
         0    0.2500         0    0.2500         0    0.1667    0.3333

% #2 - Crawling the web from site 1 and printing "location" at timestep 1
% and 2
echo off

x1 =

    0.2500
    0.2500
    0.2500
    0.2500
         0
         0
         0


x2 =

    0.1250
    0.1250
    0.1250
    0.2500
    0.1875
    0.0625
    0.1250

% #3 - printing location of crawlers starting at site one for different
% time steps using for loop
echo off 
x5 
    0.0888
    0.1694
    0.0639
    0.1401
    0.2474
    0.1412
    0.1491

x10 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x20 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x50 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

% #4 - changing starting vector to being equally distributed and repeating
% process from #3 to observe changes in behavior based on new location
% vectors. 
echo off 
x1 
    0.8333
    1.0000
    0.6667
    1.1667
    1.5833
    0.7500
    1.0000

x2 
    0.7083
    1.1111
    0.5000
    1.0417
    1.6944
    0.9444
    1.0000

x10 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x20 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x50 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

% #5 - computing steady state q of probability matrix, aka the page rank
% vector. q is then normalized to sum to 7, the number of sites in our mini
% web. The site with the highest page rank is site 5, while the one with
% the lowest page rank is site 3. 
echo off 

q =

    0.6123
    1.2010
    0.4240
    0.9655
    1.7484
    0.9891
    1.0596

5 has the highest page rank 
3 has the lowest page rank 
% #6 - finding how we get highest page rank
% 6a. finding in degree nodes by multiplying adjacency matrix by column
% vector of ones (sums of each row) 
% 6b. Finding out degree nodes by getting sums of each row
% 6c. sorting page rank and din and dout in descending order 
% seems to show that the page rank has a strong positive association with
% the number of in degree nodes and a slight negative association with out
% degree nodes 
echo off 

din =

     3
     4
     3
     5
     6
     3
     4


dout =

     4     4     4     4     3     6     3


qsorted =

    1.7484
    1.2010
    1.0596
    0.9891
    0.9655
    0.6123
    0.4240


Ivec =

     5
     2
     7
     6
     4
     1
     3


din_sorted =

     6
     4
     4
     3
     5
     3
     3


dout_sorted =

     3     4     3     6     4     4     4

% #7 - The site with the lowest page rank is site 3, as discussed earlier.
% Therefore, to raise the page rank of this website, we will add an edge
% between the 5th website. with the highest page rank, and the 3rd website,
% as this would increase the in degree nodes while connecting from the most
% popular website in our mini-web. From the results, we can see that the
% page rank of the 3rd website goes from last to 4th, meaning this change
% worked. Below. I print the steady state vector r and the sorted version.  
echo off 

r =

    0.2698
    0.3469
    0.3469
    0.3854
    0.5781
    0.2891
    0.3469


rsorted =

    0.5781
    0.3854
    0.3469
    0.3469
    0.3469
    0.2891
    0.2698


Pvec =

     5
     4
     2
     3
     7
     6
     1

% #8 - Adding a damping factor and finding steady state vectors for
% resulting P-hat matrices. The results show that adding a higher dampening
% factor results in the pagerank values getting closer, which concurs with
% the idea of web crawlers choosing sites randomly. 
echo off 

ans =

    0.1017
    0.1617
    0.0792
    0.1442
    0.2294
    0.1364
    0.1476


ans =

    0.0925
    0.1679
    0.0671
    0.1404
    0.2427
    0.1394
    0.1499


ans =

    0.1096
    0.1567
    0.0899
    0.1468
    0.2170
    0.1343
    0.1459

Nilabh_lab3_script
% #1 - defining a mini web 
echo off 

A =

     1     0     1     0     0     0     1
     1     1     0     0     1     1     0
     1     0     1     0     0     1     0
     1     1     1     1     0     1     0
     0     1     1     1     1     1     1
     0     0     0     1     1     1     0
     0     1     0     1     0     1     1


P =

    0.2500         0    0.2500         0         0         0    0.3333
    0.2500    0.2500         0         0    0.3333    0.1667         0
    0.2500         0    0.2500         0         0    0.1667         0
    0.2500    0.2500    0.2500    0.2500         0    0.1667         0
         0    0.2500    0.2500    0.2500    0.3333    0.1667    0.3333
         0         0         0    0.2500    0.3333    0.1667         0
         0    0.2500         0    0.2500         0    0.1667    0.3333

% #2 - Crawling the web from site 1 and printing "location" at timestep 1
% and 2
echo off

x1 =

    0.2500
    0.2500
    0.2500
    0.2500
         0
         0
         0


x2 =

    0.1250
    0.1250
    0.1250
    0.2500
    0.1875
    0.0625
    0.1250

% #3 - printing location of crawlers starting at site one for different
% time steps using for loop
echo off 
x5 
    0.0888
    0.1694
    0.0639
    0.1401
    0.2474
    0.1412
    0.1491

x10 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x20 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

x50 
    0.0875
    0.1716
    0.0606
    0.1379
    0.2498
    0.1413
    0.1514

% #4 - changing starting vector to being equally distributed and repeating
% process from #3 to observe changes in behavior based on new location
% vectors. 
echo off 
x1 
    0.8333
    1.0000
    0.6667
    1.1667
    1.5833
    0.7500
    1.0000

x2 
    0.7083
    1.1111
    0.5000
    1.0417
    1.6944
    0.9444
    1.0000

x10 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x20 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

x50 
    0.6123
    1.2010
    0.4239
    0.9655
    1.7485
    0.9891
    1.0597

% #5 - computing steady state q of probability matrix, aka the page rank
% vector. q is then normalized to sum to 7, the number of sites in our mini
% web. The site with the highest page rank is site 5, while the one with
% the lowest page rank is site 3. 
echo off 

q =

    0.6123
    1.2010
    0.4240
    0.9655
    1.7484
    0.9891
    1.0596

5 has the highest page rank 
3 has the lowest page rank 
% #6 - finding how we get highest page rank
% 6a. finding in degree nodes by multiplying adjacency matrix by column
% vector of ones (sums of each row) 
% 6b. Finding out degree nodes by getting sums of each row
% 6c. sorting page rank and din and dout in descending order 
% seems to show that the page rank has a strong positive association with
% the number of in degree nodes and a slight negative association with out
% degree nodes 
echo off 

din =

     3
     4
     3
     5
     6
     3
     4


dout =

     4     4     4     4     3     6     3


qsorted =

    1.7484
    1.2010
    1.0596
    0.9891
    0.9655
    0.6123
    0.4240


Ivec =

     5
     2
     7
     6
     4
     1
     3


din_sorted =

     6
     4
     4
     3
     5
     3
     3


dout_sorted =

     3     4     3     6     4     4     4

% #7 - The site with the lowest page rank is site 3, as discussed earlier.
% Therefore, to raise the page rank of this website, we will add an edge
% between the 5th website. with the highest page rank, and the 3rd website,
% as this would increase the in degree nodes while connecting from the most
% popular website in our mini-web. From the results, we can see that the
% page rank of the 3rd website goes from last to 4th, meaning this change
% worked. Below. I print the steady state vector r and the sorted version.  
echo off 

r =

    0.2698
    0.3469
    0.3469
    0.3854
    0.5781
    0.2891
    0.3469


rsorted =

    0.5781
    0.3854
    0.3469
    0.3469
    0.3469
    0.2891
    0.2698


Pvec =

     5
     4
     2
     3
     7
     6
     1

% #8 - Adding a damping factor and finding steady state vectors for
% resulting P-hat matrices. The results show that adding a higher dampening
% factor results in the pagerank values getting closer, which concurs with
% the idea of web crawlers choosing sites randomly. 
echo off 

q_85 =

    0.7116
    1.1319
    0.5542
    1.0092
    1.6055
    0.9546
    1.0329


q_95 =

    0.6478
    1.1756
    0.4700
    0.9825
    1.6990
    0.9759
    1.0493


q_75 =

    0.7671
    1.0966
    0.6293
    1.0276
    1.5187
    0.9398
    1.0210

pagerank_network
{Error using <a href="matlab:matlab.internal.language.introspective.errorDocCallback('load')" style="font-weight:bold">load</a>
Unable to read file 'facebook'. No such file or directory.

Error in <a href="matlab:matlab.internal.language.introspective.errorDocCallback('pagerank_network', '/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m', 2)" style="font-weight:bold">pagerank_network</a> (<a href="matlab: opentoline('/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m',2,0)">line 2</a>)
load facebook
} 
pagerank_network
{Error using <a href="matlab:matlab.internal.language.introspective.errorDocCallback('load')" style="font-weight:bold">load</a>
Unable to read file 'twitter'. No such file or directory.

Error in <a href="matlab:matlab.internal.language.introspective.errorDocCallback('pagerank_network', '/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m', 2)" style="font-weight:bold">pagerank_network</a> (<a href="matlab: opentoline('/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m',2,0)">line 2</a>)
load twitter
} 
pagerank_network
{Error using <a href="matlab:matlab.internal.language.introspective.errorDocCallback('load')" style="font-weight:bold">load</a>
Unable to read file 'twitter'. Input cannot be a directory.

Error in <a href="matlab:matlab.internal.language.introspective.errorDocCallback('pagerank_network', '/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m', 2)" style="font-weight:bold">pagerank_network</a> (<a href="matlab: opentoline('/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m',2,0)">line 2</a>)
load twitter
} 
pagerank_network
{Error using <a href="matlab:matlab.internal.language.introspective.errorDocCallback('load')" style="font-weight:bold">load</a>
Unable to read file 'twitter'. Input cannot be a directory.

Error in <a href="matlab:matlab.internal.language.introspective.errorDocCallback('pagerank_network', '/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m', 2)" style="font-weight:bold">pagerank_network</a> (<a href="matlab: opentoline('/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m',2,0)">line 2</a>)
load twitter
} 
pagerank_network
{Error using <a href="matlab:matlab.internal.language.introspective.errorDocCallback('load')" style="font-weight:bold">load</a>
Unable to read file 'twitter'. Input cannot be a directory.

Error in <a href="matlab:matlab.internal.language.introspective.errorDocCallback('pagerank_network', '/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m', 2)" style="font-weight:bold">pagerank_network</a> (<a href="matlab: opentoline('/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m',2,0)">line 2</a>)
load twitter
} 
pagerank_network
{Error using <a href="matlab:matlab.internal.language.introspective.errorDocCallback('load')" style="font-weight:bold">load</a>
Unable to read file 'twitter'. Input cannot be a directory.

Error in <a href="matlab:matlab.internal.language.introspective.errorDocCallback('pagerank_network', '/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m', 2)" style="font-weight:bold">pagerank_network</a> (<a href="matlab: opentoline('/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m',2,0)">line 2</a>)
load twitter
} 
pagerank_network
{Error using <a href="matlab:matlab.internal.language.introspective.errorDocCallback('load')" style="font-weight:bold">load</a>
Unable to read file 'twitter'. No such file or directory.

Error in <a href="matlab:matlab.internal.language.introspective.errorDocCallback('pagerank_network', '/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m', 2)" style="font-weight:bold">pagerank_network</a> (<a href="matlab: opentoline('/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m',2,0)">line 2</a>)
load twitter
} 
pagerank_network
{Error using <a href="matlab:matlab.internal.language.introspective.errorDocCallback('load')" style="font-weight:bold">load</a>
Unable to read file 'twitter.tar.gz'. Input must be a MAT-file or an ASCII file
containing numeric data with same number of columns in each row.

Error in <a href="matlab:matlab.internal.language.introspective.errorDocCallback('pagerank_network', '/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m', 2)" style="font-weight:bold">pagerank_network</a> (<a href="matlab: opentoline('/Users/vedantnilabh/Documents/MATLAB/pagerank_network.m',2,0)">line 2</a>)
load twitter.tar.gz
} 
pagerank_network
pagerank_network
Exception "java.lang.ClassNotFoundException: com/intellij/codeInsight/editorActions/FoldingData"while constructing DataFlavor for: application/x-java-jvm-local-objectref; class=com.intellij.codeInsight.editorActions.FoldingData
Exception "java.lang.ClassNotFoundException: com/intellij/codeInsight/editorActions/FoldingData"while constructing DataFlavor for: application/x-java-jvm-local-objectref; class=com.intellij.codeInsight.editorActions.FoldingData
